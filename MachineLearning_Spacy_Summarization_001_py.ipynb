{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning Spacy - Summarization 001.py",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcaires2/ResumoTextoSpacyExtraction/blob/master/MachineLearning_Spacy_Summarization_001_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPi_fnLrlxGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDn09UqLlN6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from pathlib import Path\n",
        "nlp= spacy.load('pt_core_news_sm')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrF46f4fi5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.pt.stop_words import STOP_WORDS\n",
        "stop_words=list(STOP_WORDS)\n",
        "stopWords =['arq', 'identificador', '419', 'ref', 'validação', '2006', '2001', '200', 'jus', '0014', 'mov', 'rua', 'resolução', 'https', 'das', 'deste', 'processo', 'lei', 'conforme', 'autora', 'dos', 'inicial', 'petição', 'não', 'para', 'com', 'por', 'projudi', 'que']\n",
        "stopWords2 =['documento','tjpr','juntada','assinado','digitalmente','petição','inicial','como','seus','pelo','pela','também','todos','todas','pois','ação','deve','parte','seja']\n",
        "stopWords3 =['assim','pelos','ainda','prova','código','mais','caso','demanda','são','nos','seu','sem','aos','uma','ser','sua','cep','pr','nº']\n",
        "stop_words = stopWords + stopWords2 +stopWords3 + stop_words\n",
        "#print(stop_words)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjErXqVwcZnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ArquivoCaminhoAbsoluto =Path(r\"/content/drive/My Drive/Colab Notebooks/MachineLearning/MachineLearningLDA/ExtravioBagagem/DadosTXT/ModeloLimpo.txt\")\n",
        "ArquivoDiretorioResumoDestino=Path(r\"/content/drive/My Drive/Colab Notebooks/MachineLearning/ResumosMachineLearning\")\n",
        "ArquivoNomePuro =Path(ArquivoCaminhoAbsoluto).stem\n",
        "\n",
        "def extrair_txt(filename):\n",
        "    \n",
        "    #with open(filename, 'r', encoding = \"ISO-8859-1\") as f:\n",
        "    with open(filename,'r',encoding='utf-8')as f: # se extrair o txt direto pelo word ou em sites online use esta linha, se o txt foi extraído de pdf usar o encoding de cima...\n",
        "       temp_text = f.read()\n",
        "              \n",
        "    return temp_text\n",
        "\n",
        "texto = extrair_txt(ArquivoCaminhoAbsoluto)\n",
        "doc=nlp(texto)\n",
        "corpus = [sent.text.lower() for sent in doc.sents ]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of-ycUIeJYtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(stop_words=stop_words)   \n",
        "cv_fit=cv.fit_transform(corpus)    \n",
        "word_list = cv.get_feature_names();    \n",
        "count_list = cv_fit.toarray().sum(axis=0) # contar quantas vezes cada palavra que não caiu na lista de irrelevância aparece no corpus documento (já separado por sentenças)\n",
        "word_frequency = dict(zip(word_list,count_list)) # dicionario formado por cada palavra e nr vezes que aparece ex {'00': 11, '000': 13, '0001': 1, '0009673': 21...} pair kv"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5G9MJ1ojhFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(word_list)\n",
        "# print(count_list)\n",
        "# print(word_frequency)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub6sdLsykpoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val=sorted(word_frequency.values())\n",
        "higher_word_frequencies = [word for word,freq in word_frequency.items() if freq in val[-5:]]\n",
        "# print(\"\\nWords with higher frequencies: \", higher_word_frequencies)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzn_juBGmQJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets relative frequency of words\n",
        "# retiro o nr de vezes que uma palavra apareceu e este passa a ser o valor 1 divisor e todas as outras terão o valor delas dividido por este numero ex 10/52 = 0.19230769\n",
        "higher_frequency = val[-1] #52\n",
        "\n",
        "for word in word_frequency.keys():  \n",
        "    word_frequency[word] = (word_frequency[word]/higher_frequency) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dffpinF9mtwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Rank de Sentenças -> com os valore relativos de cada palavra vou encontrar o valor da sentença -> soma dos valores relativos de cada uma das palavras que a compõe\n",
        "#\n",
        "\n",
        "\n",
        "sentence_rank={}\n",
        "for sent in doc.sents:\n",
        "    for word in sent :       \n",
        "        if word.text.lower() in word_frequency.keys():            \n",
        "            if sent in sentence_rank.keys():\n",
        "                sentence_rank[sent]+=word_frequency[word.text.lower()]\n",
        "            else:\n",
        "                sentence_rank[sent]=word_frequency[word.text.lower()]\n",
        "\n",
        "\n",
        "#print(sentence_rank)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-UBITSGnyJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#\n",
        "#Esta parte é interessante porque faço um rank matemático de sentenças no que tinge a soma dos valores relativos de suas palavras mas não modifico a ordem que elas aparecem no texto\n",
        "#com isso supero questão de desorganizar a narrativa.\n",
        "#[12.249999999999998, 8.192307692307692, 7.557692307692306, 7.499999999999997, 7.288461538461538, 7.153846153846152, 6.711538461538462, 6.269230769230768, 6.211538461538461, 6.153846153846154]\n",
        "\n",
        "\n",
        "top_sentences=(sorted(sentence_rank.values())[::-1]) #- ordem decrescente\n",
        "top_sent=top_sentences[:5]\n",
        "# print(top_sent)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Rb4LwrpUyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9f4b873f-2e99-461f-aee2-9999c3048c38"
      },
      "source": [
        "texto_resumido=[]\n",
        "for sent,strength in sentence_rank.items():  \n",
        "    if strength in top_sent: # se a soma relativa das palavras de uma sentença estiver no top_sent\n",
        "        texto_resumido.append(sent)\n",
        "    else:\n",
        "        continue\n",
        "for item in texto_resumido:\n",
        "    print(item,end=\" \")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contudo, os autores sofreram com a desorganização e o péssimo serviço prestado pela companhia aérea ré, vejamos: no dia 23/12/2019, por se tratar de um trecho internacional, o casal de autores chegou ao aeroporto de Lisboa (PT) às 16h, ou seja, duas horas antes do horário de saída do voo. Porém, os autores novamente foram surpreendidos, uma vez que visualizaram no painel de voos do aeroporto, que o único voo atrasado era o da companhia ré, responsável pelo trecho Lisboa (PT) – Paris (FR) - VOO TP 442 -, sendo que o novo horário de saída já estava para às 11h55min. Inconformados, o casal de autores e mais alguns brasileiros que também receberam essa informação, até argumentaram que as bagagens estavam dentro dos limites de peso para serem consideradas bagagens de mão, sendo que até se propuseram a pesar as bagagens para provarem que estavam dentro dos padrões, porém a companhia aérea ré apenas comunicou que não tinha o medidor adequado no local e que se os autores não despachassem as bagagens de mão, não embarcariam no voo. Além disso, vale destacar que em todos os momentos que o voo para Paris (FR) sofreu algum atraso ou cancelamento, a única justificativa da ré  era de que estava enfrentando “problemas operacionais”, o que por óbvio não exclui sua responsabilidade, pois são questões de fortuito interno e inerentes a atividade aérea desenvolvida pela companhia aérea ré. Repudia-se, desde já, a alegação futura da ré no tocante de  inexistência do dano moral, ou ainda que o atraso de voo e extravio de bagagem em voo internacional se tratam de meros dissabores cotidianos, pois, há de se lembrar que os autores vivenciaram momentos de profundo estresse psicológico na viagem que teria tudo para proporcionar-lhes "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcFgd-BBvItv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e99219b5-019f-43a6-9125-e2fa9e64f109"
      },
      "source": [
        "#\n",
        "#Gerando Documento Word com o Texto de Resumo\n",
        "#\n",
        "\n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "import os\n",
        "\n",
        "ArquivoDiretorioResumoDestino=Path(r\"/content/drive/My Drive/Colab Notebooks/MachineLearning/ResumosMachineLearning\")\n",
        "os.chdir(ArquivoDiretorioResumoDestino)\n",
        "\n",
        "# quebras de sentenças por paragráfos\n",
        "document = Document()\n",
        "NomeArquivoSaida = 'Resumo ' + ArquivoNomePuro + '.doc'\n",
        "document.add_heading(f'ResumoGeradoExtração - {NomeArquivoSaida}', 0)\n",
        "\n",
        "p = document.add_paragraph(' ')\n",
        "p = document.add_paragraph(' ')\n",
        "\n",
        "contador = 0\n",
        "for item in texto_resumido:\n",
        "  p = document.add_paragraph('Trecho ' + str(contador) + ': ' + str(item))\n",
        "  p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "  contador = contador +1\n",
        "\n",
        "\n",
        "p = document.add_paragraph(' ')\n",
        "p = document.add_paragraph(' ')\n",
        "\n",
        "p = document.add_paragraph(' ')\n",
        "p.add_run('ResumoGeradoExtração Continua ').underline = True\n",
        "p = document.add_paragraph(' ')\n",
        "\n",
        "\n",
        "t_continua =''\n",
        "for item in texto_resumido:\n",
        "    t_continua = t_continua + ' ' + str(item)\n",
        "p = document.add_paragraph(t_continua)\n",
        "p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
        "\n",
        "\n",
        "\n",
        "document.save(NomeArquivoSaida)\n",
        "print(' (: ')\n",
        "print('Arquivo WORD.DOC  Gerado com Sucesso')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.6/dist-packages (0.8.10)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            " (: \n",
            "Arquivo WORD.DOC  Gerado com Sucesso\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}